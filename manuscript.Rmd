---
title             : "Assessing rigor and impact of research software for hiring and promotion in psychology: A comment on Gärtner et al. (2022)"
#subtitle          : "Commentary: 'Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology'"
shorttitle        : "Assessing research software"

author: 
  - name          : "Andreas M. Brandmaier"
    affiliation   : "1,2,3"
    corresponding : no    # Define only one corresponding author
    address       : "Rüdesheimer Str. 50, 14197 Berlin"
    email         : "andreas.brandmaier@medicalschool-berlin.de"
  - name          : "Maximilian Ernst"
    affiliation   : "2,4"
  - name          : "Aaron Peikert"
    affiliation   : "2,3,4"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, MSB Medical School Berlin, Berlin, Germany"
  - id            : "2"
    institution   : "Center for Lifespan Psychology, Max Planck Institute for Human Development, Berlin, Germany"
  - id            : "3"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research, Berlin, Germany"
  - id            : "4"
    institution   : "Max Planck School of Cognition, Leipzig, Germany"
  - id            : "5"
    institution   : "Department of Imaging Neuroscience, University College London, London, UK"

    
correspondence: |
  brandmaier@mpib-berlin.mpg.de

abstract: Based on four principles of a more responsible research assessment in academic hiring and promotion processes, @gartner2022responsible suggested an evaluation scheme for published manuscripts, reusable data sets, and research software. This commentary responds to the proposed indicators for the evaluation of research software contributions in academic hiring and promotion processes. Acknowledging the significance of research software as a critical component of modern science, we propose that an evaluation scheme must emphasize the two major dimensions of rigor and impact. Generally, we believe that research software should be recognized as valuable scientific output in academic hiring and promotion, with the hope that this incentivizes the development of more open and better research software.
  
keywords          : "research software, open science, metrics, rigor, impact"
wordcount         : "`r tryCatch(wordcountaddin::word_count(here::here('manuscript.Rmd')))`"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no
csquotes          : true
babel-lang        : american
classoption       : "jou,a4paper"
header-includes:
  - |
    % Change font for whole document
    \renewcommand{\familydefault}{\rmdefault}
    \renewcommand*\rmdefault{bch}
    
    \usepackage[english,all,portrait,draft]{draftcopy}
    \usepackage[blocks]{authblk}
    
    % Add watermark for preprints at the top of each page
    % Comment out to remove preprint watermark
    %\usepackage{draftwatermark}
    %\SetWatermarkText{PREPRINT}
    %\SetWatermarkScale{0.3}
    %\SetWatermarkAngle{0}
    %\SetWatermarkVerCenter{25pt}
    %\SetWatermarkColor[rgb]{0.85,0.85,0.95}
    
    % Add header with journal information
    \usepackage{fancyhdr}
    \renewcommand{\headrulewidth}{0pt} 	% remove vertical line
    \setlength{\headheight}{54pt}  % increase vertical space for header
    \fancypagestyle{firststyle}
    {
       \fancyhf{} % clear all header and footer fields
       
    		\fancyhead[L]{\scriptsize  
    		\textit{Meta-Psychology}, 2023, vol 7, MP.201X.xxx\\\href{https://doi.org/10.15626/MP.}{https://doi.org/10.15626/MP.}\\Article type: \\Published under the CC-BY4.0 license \\
    		\smallskip
    %%%%%%%%%%%%%%%%%%%%%%%%%%%% Badges %%%%%%%%%%%%%%%%%%%%%
    		\includegraphics[scale=0.2]{badges/opendata.png}
    		\includegraphics[scale=0.2]{badges/openmaterial.png}
    		\includegraphics[scale=0.2]{badges/preregestered.png}
    		\includegraphics[scale=0.2]{badges/preregesteredplus.png}
    		}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%		
        \fancyhead[C]{\scriptsize 
            Open data: Yes\\
            Open materials: Yes \\
            Open and reproducible analysis:\\
            Open reviews and editorial process: Yes\\
            Preregistration: 
            \vspace{0.5ex}
    \\
    		\smallskip}
        \fancyhead[R]{\scriptsize   Edited by: \\
            Reviewed by: \\
            Analysis reproduced by: \\
            All supplementary files can be accessed at OSF: \\
            \href{https://doi.org/10.17605/OSF.IO/}{https://doi.org/10.17605/OSF.IO/}
            \vspace{0.5ex}
    \\
    		\smallskip}
    }
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \usepackage[]{graphicx}
    \usepackage[]{color}
    \usepackage{tikz}  % for watermark on first page
    
    \usepackage{alltt}
    \usepackage[T1]{fontenc}
    \usepackage[utf8]{inputenc}
    
    \usepackage{amsmath}  	% for advanced math displays
    \usepackage{booktabs}		% booktables
    
    % With that package, tables are single spaced
    \usepackage{setspace}
    
    \usepackage{listings}  % for inline code blocks
    \lstset{
    	basicstyle=\footnotesize\ttfamily, % the size of the fonts that are used for the code
    	breaklines=true,  % sets automatic line breaking
    	breakatwhitespace=true % sets if automatic breaks should only happen at whitespace
    }
    
    \usepackage{url}	% links to headings
    \usepackage{placeins} 	% flushes Figures before the next section starts
    % use command \FloatBarrier
    
    %% APA-style citations
    \usepackage{csquotes}
    \usepackage[backend=biber,style=apa,hyperref=true,giveninits,uniquename=init]{biblatex}
    \DeclareLanguageMapping{american}{american-apa}
    
    % TODO: Users must add their .bib file here
    \addbibresource{references.bib}
    
    % remove unwanted fields from .bib-file
    \AtEveryBibitem{	
      \clearfield{day}
      \clearfield{month}
      \clearfield{labelday}
      \clearfield{labelmonth}
    }
    %\usepackage{multicol} %unneeded, apa6 already split cols
    \raggedbottom
    
    \usepackage{hyperref} % should be loaded last as it redefines many Latex commands
    \hypersetup{colorlinks, urlcolor=blue, citecolor=blue}

documentclass     : "apa7"
output            :
  papaja::apa6_pdf:
    citation_package: biblatex
equal_contrib: yes
repro:
  packages:
    - papaja
    - here
    - aaronpeikert/repro@5075336
    - benmarwick/wordcountaddin@e075baa
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\thispagestyle{firststyle}
 

# Introduction

Based on four principles of a more responsible research assessment in academic hiring and promotion processes [@schonbrodt2022responsible], @gartner2022responsible suggested a concrete evaluation scheme for published manuscripts, reusable data sets, and research software.
We strongly support the increased emphasis on research software as a creditable and commendable scientific contribution.
Why are research software contributions important scientific contributions?
We would like to respond with a quote from the Science Code Manifesto [@manifesto]:
"Software is a cornerstone of science. Without software, twenty-first century science would be impossible"
and
"software is an essential research product, and the effort to produce, maintain, adapt, and curate code must be recognized."
However, despite the heavy reliance on computational infrastructure, the current academic infrastructure does not adequately incentivize software development and, specifically, good software engineering practice [@baxter2012research].
In line with principle 3 of @schonbrodt2022responsible, we suggest that criteria for research software contributions must capture two major dimensions: rigor and impact.
Impact measures whether the scholarly effort, implementation, and dissemination actually had a visible effect on the field.
Rigor means implementing high standards and best practices for ensuring transparency, correctness, and re-usability of a piece of software.
By setting a high bar of rigor in research software assessment in academic hiring and promotion, we hope to foster the creation of better software and, thus, better science.
From this perspective, we comment on some indicators of the proposed evaluation scheme for research software contributions [Table 3 of @gartner2022responsible].

# Proposed Criteria

## ID 9: Citations and ID 5: Date of first fully functional public version

Considering citations in relation to age of software seems to be inconsistent with the proposal of @schonbrodt2022responsible, who reminded us that a core principle of the implementation of DORA is to "abandon the use of invalid quantitative metrics of research quality and productivity in hiring and promotion" (p.2).
It is unclear why a citation-based metric for software would be more valid than the equivalent for articles.
In fact, citations suffer from additional shortcomings when used to evaluate the impact of research software, particularly when used comparatively in the context of hiring and promotion.
For example, citing data analysis packages is much more commonly accepted than citing supporting packages [such as papaja, @R-papaja, used to render this article].
Further, functionality of successful, modular scientific software is ideally reused in other software packages to avoid code duplication and enable faster development of new software.
While we highly encourage reuse from both a software engineering perspective and for scientific progress, it challenges the validity of citation-based metrics for impact.
For example, consider the `NLopt` optimization suite, which counts 1,711 citations on Google scholar at the time of writing.
`NLopt` is a backbone for many scientific packages both because it implements various optimization algorithms but also because it is open-source and can inspire re-implementations of these algorithms.
One such example is the famous `lme4` package [@bates2014fitting] for generalized linear mixed-effects models, which is partly based on `NLopt`.
`lme4` has more than 58,000 citations on Google scholar, yet it is unlikely that researchers will cite the underlying optimization algorithm.
For another example, the `pdc` package [@pdc-paper] offers functions to cluster time series based on one specific algorithm.
The `TSclust` package [@TSclust] is a wrapper package, which imports and makes accessible functionality from the `pdc` package as well as various other clustering approaches, which is very useful from a users' perspective; however, we noticed that researchers now cite `TSclust` instead of `pdc`.

## ID 6: Date of most recent substantive update

Both ID 5 and 6 are difficult to ascertain because it is not always clear when updates are considered 'substantive' or software 'fully functional'.
To assess active maintenance as an aspect of rigor, we propose to provide a simple check box, in which the author indicates whether a scientific software package is actively maintained (e.g., the software has a regular release cycle or an update within the last six months).
In addition, authors have a chance to explain why their active maintenance may be different from these guidelines.

## ID 14: Lines of Code

We discourage the lines of code (LOC) metric to measure effort.
LOC highly depends on programming language, mastery, and personal programming style.
In particular, many LOC may simply mean that a researcher writes inefficient and repetitive code, one of the great sins of programming.
On the contrary, a feature of good software is modularity because it enables reusing functions both inside and outside the project, resulting in fewer LOC.

## ID 7: Contributor Roles and Involvement

We support the standardized assessment of project contributor roles, similar to the Contributor Roles Taxonomy (CRediT; https://credit.niso.org). 
However, we like to point out that the current evaluation schemes yields different scores for the same effort of the individual researcher depending on the size of the software project.

## ID 8: License

At present, whether a piece of software is open source is not evaluated at the pre-screening stage.
However, an open license is central for assessing both rigor and (potential) impact, and should be part of the phase I assessment.
Above, we already discussed the different ways research software can have an impact --- not only by direct usage, but also by reusing software in other packages.
Open-source software makes broader impact more likely and is a prerequisite for full transparency and reproducibility [@peikert2021reproducible].
In addition, many aspects of rigor are impossible to evaluate for closed-source software --- for example, whether it is well-tested or bugs have been fixed.
Giving more weight to open licenses aligns well with the original article's emphasis on open science.
Therefore, we propose to penalize software if it does not adhere to an open-source license (those approved in a review process by the [Open Source Initiative](https://opensource.org/licenses)), for example, by allowing them only half of the total achievable points.


## ID 17: Reusability Indicator

This is one of only two criteria used in the pre-screening phase of the proposal and therefore is of central importance.
It is also important because it assesses aspects of rigor in software development: documentation, active maintenance, and testing.
However, by incorporating the size of the user base, it confounds *usability* (as an aspect of rigor) with *usage* (as an aspect of impact).
In addition, the criteria for the different proposed categories are not clearly defined.
For example, the difference between "fairly extensive" and "extensive" documentation is unclear.
As a result, this indicator is more of a "gut-feeling" indicator, roughly assessing the "size" of the software project.
Instead, in the following we propose to assess rigor and impact independently as primary aspects of a software contribution.


# Assessing Rigor

<!--In the following, we discuss aspects that we believe should be scored in phase one of the assessment.-->
We propose to use the following aspects as equally weighted indicators of rigor instead of the proposed broader re-usability indicator (item 17 in Table 3 of the proposal).
<!-- some introductory text missing -->
<!-- maybe cite
https://joss.readthedocs.io/en/latest/review_criteria.html
and
https://joss.readthedocs.io/en/latest/review_checklist.html
here -->

## Tests

Tests are essential to discover incorrect functionality, investigate code scalability and reveal poor design choices.
There are a variety of useful tests, such as unit tests of sub components or tests of software functionality at a larger scale [e.g., see the `testthat` package in R, @wickham2011].
It is possible to quantify aspects of software testing, for example, by assessing code coverage, defined as the percentage of code lines executed during testing.
However, we believe that we should give points for software that promises that major functionality is covered by tests.
Those tests should be automated or at least open-source and reproducible.

## Documentation

Just like for tests, there are different types of documentation.
For example, tutorials showcase software usage with examples, and there is application programming interface (API) documentation for individual functions and classes to enable reusing functions in other software packages.
We propose to identify relevant categories of documentation for research software, e.g., installation instructions, tutorials, API, and community guidelines (also see https://joss.readthedocs.io/en/latest/review_checklist.html), and score the presence of each of them separately.

## Maintenance

Maintaining a software package is often more work than writing it.
This should be reflected in the assessment procedure.
We propose to score two aspects of maintenance separately, maintaining the code base (such as active bug fixing and documenting changes in logs) and maintaining the community (such as providing the possibility to report bugs, feature requests, or support requests via tickets or mailing lists).

# Measuring Impact

Total citation metrics, number of users, downloads per month, GitHub stars and similar may provide a coarse measure for the impact of a software package, even though it is important to note the shortcomings we described above.

We believe that the suggested merit statement is most useful to assess impact of research software and that this should be the primary statement for committee members to evaluate if they are less concerned with the technical aspects of research software development.
In our view, researchers should be requested to indicate at least one (and up to three) research projects that directly benefited from their software contributions.
In case of recently published research software, the merit statement can be used alternatively to describe expected benefits to the field or potential use-cases of the software.
We believe this to be a fair assessment of the actual impact the specific contribution of the individual researcher had.


# Summary

In sum, we are thankful to @gartner2022responsible for highlighting the importance of research software contributions as scientifically valuable products.
We believe the current proposal should aim to better reflect the distinction between rigor and impact for software, similar to the guidelines proposed to evaluate journal articles.
To this end, we suggest a more fine-grained assessment of rigor, and put emphasis on the merit statement, in which software authors should argue in how far their project impacted other scientific endeavors.
Last, we hope to arrive at an evaluation scheme that incentivizes the development of scientific open-source software.
Note that the requirements of a given academic position should guide the weighting of research software when evaluating and selecting candidates.
In some cases, research software development may be central to a position and, therefore, a requirement (such as when hiring professors of statistics or research methods), but in other cases, we hope that research software development activities at least are considered as research output comparable to published papers.

## Author Contact

Correspondence concerning this article should be addressed to Andreas M. Brandmaier, Rüdesheimer Str. 50, 14197 Berlin.
E-mail: \href{mailto:andreas.brandmaier@medicalschool-berlin.de}{\nolinkurl{andreas.brandmaier@medicalschool-berlin.de}}

## Conflict of Interest and Funding

We have no conflict of interest to declare.
There was no specific funding for this research.

## Author Contributions

The authors made the following contributions.
Andreas M. Brandmaier
: Conceptualization, Writing - Original Draft Preparation, Writing - Review & Editing, Supervision;
Maximilian Ernst
: Conceptualization, Writing - Original Draft Preparation, Writing - Review & Editing;
Aaron Peikert
: Conceptualization, Writing - Original Draft Preparation, Writing - Review & Editing.

## Open Science Practices

 \includegraphics[scale=0.50]{badges/opendata.png}
 \includegraphics[scale=0.50]{badges/openmaterial.png}
 \includegraphics[scale=0.50]{badges/preregestered.png}
 \includegraphics[scale=0.50]{badges/preregesteredplus.png}
 
\textbf{NOTE. This will be filled out by the reproducibility team.}
This article earned the Preregistration, Open Data and the Open Materials badge for preregistering the hypothesis and analysis before data collection, and for making the data and materials openly available. It has been verified that the analysis reproduced the results presented in the article. The entire editorial process, including the open reviews, are published in the online supplement.
